# my_transformer_project

# Transformer1: From-Scratch Implementation in PyTorch  
**Author**: Yi Luo  
**Date**: 2025-04-04

---

## ğŸ“Œ Project Overview

This project is a modular, from-scratch implementation of the Transformer model architecture as introduced in the paper  
> **"Attention Is All You Need"**  
> *Ashish Vaswani et al., 2017*

The goal of this project is twofold:

1. ğŸ” **Deepen Understanding**  
   Build every component of the Transformer step-by-step in PyTorch â€” from multi-head attention and positional encoding to encoder-decoder stacks â€” to truly understand how modern attention-based models work.

2. ğŸ’¼ **Portfolio-Ready Codebase**  
   Showcase clean engineering practices and NLP knowledge for job applications in machine learning, data science, and NLP engineering roles.

---

## ğŸ§± Project Structure

This is version **`transformer1/`**, the first iteration. Future versions (e.g., `transformer2/`) may include improvements like learnable position encoding, pretrained tokenizers, and larger datasets.


transformer1/
â”œâ”€â”€ config.py                 # model hyperparameters
â”œâ”€â”€ embedding_positional.py  # token + positional encodings
â”œâ”€â”€ attention.py             # scaled dot-product & multi-head attention
â”œâ”€â”€ feed_forward.py          # position-wise FFN
â”œâ”€â”€ encoder.py               # encoder stack
â”œâ”€â”€ decoder.py               # decoder stack
â”œâ”€â”€ transformer.py           # full encoder-decoder Transformer
â”œâ”€â”€ train.py                 # training loop on toy data



---

## âš™ï¸ How to Run

### 1. Install Dependencies
```bash
pip install torch numpy tqdm

cd transformer1
python train.py




## ğŸ“Œ Project Overview

### `transformer2/` â€” Real Dataset Training & Visualization

A classifier-only Transformer encoder trained on [AG News](https://huggingface.co/datasets/ag_news). This version demonstrates:

- âš™ï¸ Hugging Face tokenizer integration
- ğŸ“Š Training on 20k news headlines for topic classification
- ğŸ§  Encoder-only Transformer architecture
- ğŸ“‰ Loss & accuracy plots (`training_plot.png`)
- ğŸ›‘ Early stopping based on validation
- ğŸ’¾ Model saving to `best_model.pt`
- ğŸ” Model structure inspection using `torchinfo` in `visualize.py`


---
## ğŸ§± Project Structure

transformer2/
â”œâ”€â”€ config.py
â”œâ”€â”€ embedding_positional.py
â”œâ”€â”€ attention.py
â”œâ”€â”€ feed_forward.py
â”œâ”€â”€ encoder.py
â”œâ”€â”€ transformer.py           # encoder-only classifier
â”œâ”€â”€ train.py                 # training with early stopping & plotting
â”œâ”€â”€ visualize.py             # model summary with torchinfo
â”œâ”€â”€ training_plot.png        # saved learning curve
â”œâ”€â”€ best_model.pt            # saved weights

---

## âš™ï¸ How to Run

### 1. Install Dependencies

```bash
pip install torch numpy tqdm transformers datasets scikit-learn matplotlib torchinfo

cd transformer2
python train.py       # trains model and plots training/validation curves
python visualize.py   # shows architecture summary
